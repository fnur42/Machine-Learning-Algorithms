{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19},{"sourceId":482,"sourceType":"datasetVersion","datasetId":228},{"sourceId":666,"sourceType":"datasetVersion","datasetId":306},{"sourceId":2184,"sourceType":"datasetVersion","datasetId":1216},{"sourceId":3651,"sourceType":"datasetVersion","datasetId":2169},{"sourceId":3987,"sourceType":"datasetVersion","datasetId":2374},{"sourceId":8204,"sourceType":"datasetVersion","datasetId":4458},{"sourceId":36825,"sourceType":"datasetVersion","datasetId":28901},{"sourceId":196262,"sourceType":"datasetVersion","datasetId":84803},{"sourceId":457469,"sourceType":"datasetVersion","datasetId":209295},{"sourceId":960570,"sourceType":"datasetVersion","datasetId":515409},{"sourceId":1043970,"sourceType":"datasetVersion","datasetId":576697},{"sourceId":1116242,"sourceType":"datasetVersion","datasetId":626341},{"sourceId":1295161,"sourceType":"datasetVersion","datasetId":748772},{"sourceId":1519136,"sourceType":"datasetVersion","datasetId":895414},{"sourceId":2968722,"sourceType":"datasetVersion","datasetId":1820018},{"sourceId":2984728,"sourceType":"datasetVersion","datasetId":1829286},{"sourceId":3003315,"sourceType":"datasetVersion","datasetId":1839803},{"sourceId":3622606,"sourceType":"datasetVersion","datasetId":2169827},{"sourceId":3701254,"sourceType":"datasetVersion","datasetId":2214347},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning with Python\n\n* Linear regression (least squares regression)\n* Logistic regression\n* Linear discriminant analysis\n* Decision trees\n* Naive Bayes (hem sÄ±nÄ±flandÄ±rma hem de regrasyon iÃ§in) / Bayes Teoremi\n* K-Nearest Neighbors (KNN)\n* Expectation Maximization (EM)\n* Support Vector Machines (SVM\n* Random Forests\n* Gradient Boosting Machines (GBM) /Gradient descent\n* Derin Ã–ÄŸrenme Modelleri (Yapay Sinir AÄŸlarÄ±)\n* KÃ¼meleme (K-Means, HiyerarÅŸik KÃ¼meleme, Gausian KarÄ±ÅŸÄ±m Modeli)\n* Maximum Likelihood/Gauss-Newton\n","metadata":{}},{"cell_type":"markdown","source":"# K-Nearest Neighbors\n\n> K-En YakÄ±n KomÅŸular (KNN), hem sÄ±nÄ±flandÄ±rma hem de regresyon gÃ¶revleri iÃ§in kullanÄ±labilen basit bir denetimli makine Ã¶ÄŸrenimi algoritmasÄ±dÄ±r.Temel fikir, bir veri noktasÄ±nÄ±n tahmin edilmesinde, o noktaya en yakÄ±n komÅŸularÄ±nÄ±n etkisini kullanmaktÄ±r.\n\n> Algoritma Ã§alÄ±ÅŸma prensibi ÅŸu adÄ±mlarÄ± izler:\n\n* EÄŸitim veri setindeki tÃ¼m veri noktalarÄ±nÄ±n Ã¶zellik vektÃ¶rleri ve etiketleri kaydedilir.\n* Bir test veri noktasÄ± verildiÄŸinde, bu noktaya en yakÄ±n k komÅŸusu bulunur.\n* SÄ±nÄ±flandÄ±rma problemlerinde, bu k komÅŸunun etiketleri incelenir ve en sÄ±k tekrar eden etiket tahmin olarak verilir.\n* Regresyon problemlerinde, bu k komÅŸunun etiketlerinin ortalamasÄ± tahmin olarak verilir.\n\n> KNN algoritmasÄ±, yeni veri noktalarÄ±nÄ± tahmin etmek iÃ§in oldukÃ§a basit ve etkili bir yÃ¶ntemdir. Ancak, bÃ¼yÃ¼k veri setlerinde hesaplama yoÄŸunluÄŸu ve bellek kullanÄ±mÄ± gibi bazÄ± zorluklarla karÅŸÄ±laÅŸabilir.\n\n**KullanÄ±lan Data Set ðŸ‘‡**\n\nhttps://www.kaggle.com/datasets/uciml/iris","metadata":{}},{"cell_type":"markdown","source":"***AÅŸaÄŸÄ±daki Kod HazÄ±r Fonksiyon KullanÄ±lmadan YapÄ±lmÄ±ÅŸtÄ±r!***","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Ä°ris veri setini yÃ¼kleme\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n# Veri kÃ¼mesinin aÃ§Ä±klamasÄ±nÄ± yazdÄ±rma\nprint(df.describe())\n\nX = iris.data\ny = iris.target\n\n# EÄŸitim ve test setlerine ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# KNN sÄ±nÄ±fÄ±nÄ± tanÄ±mlama\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n    \n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n    \n    def euclidean_distance(self, x1, x2):\n        return np.sqrt(np.sum((x1 - x2)**2))\n    \n    def predict(self, X_test):\n        predictions = []\n        for i in range(len(X_test)):\n            distances = [self.euclidean_distance(X_test[i], x_train) for x_train in self.X_train]\n            k_indices = np.argsort(distances)[:self.k]\n            k_nearest_labels = [self.y_train[j] for j in k_indices]\n            most_common = max(set(k_nearest_labels), key=k_nearest_labels.count)\n            predictions.append(most_common)\n        return predictions\n\n# KNN modelini oluÅŸturma\nknn = KNN(k=3)\nknn.fit(X_train, y_train)\n\n# Modeli test seti Ã¼zerinde deÄŸerlendirme\ny_pred = knn.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:48:23.540379Z","iopub.execute_input":"2024-02-04T13:48:23.541071Z","iopub.status.idle":"2024-02-04T13:48:23.606251Z","shell.execute_reply.started":"2024-02-04T13:48:23.541036Z","shell.execute_reply":"2024-02-04T13:48:23.605238Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"               Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\ncount  150.000000     150.000000    150.000000     150.000000    150.000000\nmean    75.500000       5.843333      3.054000       3.758667      1.198667\nstd     43.445368       0.828066      0.433594       1.764420      0.763161\nmin      1.000000       4.300000      2.000000       1.000000      0.100000\n25%     38.250000       5.100000      2.800000       1.600000      0.300000\n50%     75.500000       5.800000      3.000000       4.350000      1.300000\n75%    112.750000       6.400000      3.300000       5.100000      1.800000\nmax    150.000000       7.900000      4.400000       6.900000      2.500000\nAccuracy: 1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Logistic Regression\n\n> Lojistik regresyon, ikili baÄŸÄ±mlÄ± bir deÄŸiÅŸkeni modellemek iÃ§in lojistik bir iÅŸlev kullanan istatistiksel bir modeldir. Kategorik baÄŸÄ±mlÄ± bir deÄŸiÅŸkenin olasÄ±lÄ±ÄŸÄ±nÄ± tahmin etmek iÃ§in kullanÄ±lan denetimli bir Ã¶ÄŸrenme sÄ±nÄ±flandÄ±rma algoritmasÄ±dÄ±r.Logistic Regression, baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin aÄŸÄ±rlÄ±klarÄ±nÄ± ve etkilerini kullanarak, her bir sÄ±nÄ±fÄ±n olasÄ±lÄ±ÄŸÄ±nÄ± tahmin eder. Ancak, doÄŸrudan sÄ±nÄ±flarÄ±n tahmin edilmesi yerine, bir giriÅŸin her sÄ±nÄ±fa ait olma olasÄ±lÄ±ÄŸÄ± verilir. Bu olasÄ±lÄ±klar daha sonra bir eÅŸik deÄŸeri (threshold) kullanÄ±larak sÄ±nÄ±flara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.\n\n> Logistic Regression modeli, sigmoid fonksiyonu olarak da bilinen lojistik fonksiyon kullanÄ±larak oluÅŸturulur. Bu fonksiyon, sonsuz olan bir aralÄ±ktaki giriÅŸlerin deÄŸerini 0 ile 1 arasÄ±nda bir Ã§Ä±ktÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, bu da olasÄ±lÄ±k olarak yorumlanabilir. Logistic Regression algoritmasÄ±, parametre tahminlerini genellikle maksimum olabilirlik tahmini (maximum likelihood estimation) yÃ¶ntemiyle elde eder. Bu, veri setinin olasÄ±lÄ±ÄŸÄ±nÄ± maksimize etmek iÃ§in model parametrelerini ayarlamayÄ± iÃ§erir.","metadata":{}},{"cell_type":"markdown","source":"**KullanÄ±lan Data Set ðŸ‘‡**\n\nhttps://www.kaggle.com/datasets/dipayanbiswas/parkinsons-disease-speech-signal-features","metadata":{}},{"cell_type":"markdown","source":"***AÅŸaÄŸÄ±daki Kod HazÄ±r Fonksiyon KullanÄ±lmadan YapÄ±lmÄ±ÅŸtÄ±r!***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ndf = pd.read_csv('/kaggle/input/parkinsons-disease-speech-signal-features/pd_speech_features.csv')\n\n# Exploratory data analysis\nprint(df.shape)\n\n# BaÄŸÄ±msÄ±z ve baÄŸÄ±mlÄ± deÄŸiÅŸkenleri tanÄ±mlama\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\n# Veriyi eÄŸitim ve test setlerine ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Verileri Ã¶lÃ§eklendirme\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Lojistik regresyon modeli oluÅŸturma\nclass LogisticRegression:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.learning_rate = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n    \n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n    \n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n        for _ in range(self.n_iterations):\n            model = np.dot(X, self.weights) + self.bias\n            y_predicted = self.sigmoid(model)\n            \n            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n            db = (1 / n_samples) * np.sum(y_predicted - y)\n            \n            self.weights -= self.learning_rate * dw\n            self.bias -= self.learning_rate * db\n    \n    def predict(self, X):\n        model = np.dot(X, self.weights) + self.bias\n        y_predicted = self.sigmoid(model)\n        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n        return y_predicted_cls\n\n# Modeli eÄŸitme\nmodel = LogisticRegression(learning_rate=0.1, n_iterations=1000)\nmodel.fit(X_train, y_train)\n\n# Modeli deÄŸerlendirme\ny_pred = model.predict(X_test)\naccuracy = np.mean(y_pred == y_test)\nprint(\"Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.143265Z","iopub.status.idle":"2024-02-04T13:44:52.143713Z","shell.execute_reply.started":"2024-02-04T13:44:52.143479Z","shell.execute_reply":"2024-02-04T13:44:52.143497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Maximum Likelihood/Gauss-Newton\n\n> Maximum Likelihood ve Gauss-Newton, parametre tahmininde kullanÄ±lan iki farklÄ± yÃ¶ntemdir.\n\n**1. Maximum Likelihood (MLE - Maximum Likelihood Estimation):** \n\n> Bu yÃ¶ntem, parametre tahminindeki bir olasÄ±lÄ±k odaklÄ± yaklaÅŸÄ±mdÄ±r. Bir model belirlendiÄŸinde, MLE, gÃ¶zlemlenen veri setinin bu modele en uygun parametre deÄŸerlerini bulmaya Ã§alÄ±ÅŸÄ±r. Bu parametreler, veri setinin olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±n maksimum olasÄ±lÄ±ÄŸÄ±nÄ± saÄŸlamak iÃ§in tahmin edilir.\n\n**2. Gauss-Newton:** \n\n> Bu yÃ¶ntem, Ã¶zellikle doÄŸrusal olmayan regresyon modellerinde kullanÄ±lan bir optimizasyon yÃ¶ntemidir. Bir tahmin modelinin parametrelerini gÃ¼ncellemek iÃ§in kullanÄ±lÄ±r. Gauss-Newton yÃ¶ntemi, hedef fonksiyonunun (genellikle ortalama kareler hatasÄ±) birinci ve ikinci tÃ¼revlerini kullanarak iteratif olarak parametreleri gÃ¼nceller. Bu yÃ¶ntem, tahmin edilen parametrelerin gerÃ§ek deÄŸerlere yakÄ±nsamasÄ±nÄ± saÄŸlamak iÃ§in kullanÄ±lÄ±r.","metadata":{}},{"cell_type":"markdown","source":"**AÅŸaÄŸÄ±daki Maximum Likelihood Kodunda KullanÄ±lan Data Set ðŸ‘‡**\n\nhttps://www.kaggle.com/datasets/abhia1999/chronic-kidney-disease","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import multivariate_normal\n\n# Veri setini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme\ndf = pd.read_csv('/kaggle/input/chronic-kidney-disease/new_model.csv')\n\n# Ã–zellikler ve hedef deÄŸiÅŸkeni seÃ§me\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\n# EÄŸitim ve test setlerini ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Maximum Likelihood yÃ¶ntemi ile model oluÅŸturma\ndef log_likelihood(params, X, y):\n    mu = params[:-1]\n    sigma = np.exp(params[-1])\n    likelihood = multivariate_normal.logpdf(X, mean=mu, cov=sigma)\n    return -np.sum(likelihood)\n\n# BaÅŸlangÄ±Ã§ parametrelerini belirleme\ninitial_params = np.random.rand(X_train.shape[1] + 1)\n\n# Modeli uydurma\nresult = minimize(log_likelihood, initial_params, args=(X_train, y_train), method='Nelder-Mead')\n\n# Optimize edilmiÅŸ parametreleri alma\noptimized_params = result.x\n\n# Test seti Ã¼zerinde tahminler yapma\nmu = optimized_params[:-1]\nsigma = np.exp(optimized_params[-1])\npredicted_likelihoods = multivariate_normal.logpdf(X_test, mean=mu, cov=sigma)\n\n# Tahminleri sÄ±nÄ±flara dÃ¶nÃ¼ÅŸtÃ¼rme\npredictions = (predicted_likelihoods > 0.5).astype(int)\n\n# DoÄŸruluk hesaplama\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:28:47.344404Z","iopub.status.idle":"2024-02-04T16:28:47.344783Z","shell.execute_reply.started":"2024-02-04T16:28:47.344600Z","shell.execute_reply":"2024-02-04T16:28:47.344616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom scipy.optimize import minimize\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import norm\n\n# Veri setini yÃ¼kleme\ndf = pd.read_csv('/kaggle/input/forest-cover-type-dataset/covtype.csv')\n\n# Preprocess data\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\n# EÄŸitim ve test setlerini ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Maximum Likelihood yÃ¶ntemi ile model oluÅŸturma\ndef log_likelihood(params, X, y):\n    a, b = params\n    y_pred = np.dot(X, a) + b\n    residuals = y - y_pred\n    likelihood = -np.sum(norm.logpdf(residuals))\n    return likelihood\n\ndef log_likelihood_gradient(params, X, y):\n    a, b = params\n    y_pred = np.dot(X, a) + b\n    residuals = y - y_pred\n    gradient = np.zeros(len(params))\n    gradient[0] = np.sum(np.dot(X.T, residuals))\n    gradient[1] = np.sum(residuals)\n    return gradient\n\n\nparams_init = np.random.rand(X_train.shape[1] + 1)\nresult_ml = minimize(log_likelihood, params_init, args=(X_train.values, y_train.values), method='BFGS', jac=log_likelihood_gradient)\nparams_ml = result_ml.x\n\n# Gauss-Newton yÃ¶ntemi ile model oluÅŸturma\ndef gauss_newton(params, X, y):\n    a, b = params\n    y_pred = np.dot(X, a) + b\n    residuals = y - y_pred\n    hessian = np.zeros((len(params), len(params)))\n    hessian[0, 0] = np.sum(np.dot(X.T, X))\n    hessian[0, 1] = np.sum(X)\n    hessian[1, 0] = np.sum(X)\n    hessian[1, 1] = len(X)\n    gradient = np.zeros(len(params))\n    gradient[0] = np.sum(np.dot(X.T, residuals))\n    gradient[1] = np.sum(residuals)\n    params_new = params - np.linalg.solve(hessian, gradient)\n    return params_new\n\nparams_init = np.random.rand(X_train.shape[1] + 1)\nparams_gn = gauss_newton(params_init, X_train.values, y_train.values)\n\n# Model performansÄ±nÄ± deÄŸerlendirme\ndef evaluate_model(params, X, y):\n    a, b = params\n    y_pred = np.dot(X, a) + b\n    mse = mean_squared_error(y, y_pred)\n    return mse\n\nmse_ml = evaluate_model(params_ml, X_test.values, y_test.values)\nmse_gn = evaluate_model(params_gn, X_test.values, y_test.values)\n\nprint(\"Maximum Likelihood MSE:\", mse_ml)\nprint(\"Gauss-Newton MSE:\", mse_gn)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T16:28:40.783165Z","iopub.execute_input":"2024-02-04T16:28:40.783800Z","iopub.status.idle":"2024-02-04T16:28:47.343557Z","shell.execute_reply.started":"2024-02-04T16:28:40.783768Z","shell.execute_reply":"2024-02-04T16:28:47.341524Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gradient\n\u001b[1;32m     36\u001b[0m params_init \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m result_ml \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBFGS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_likelihood_gradient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m params_ml \u001b[38;5;241m=\u001b[39m result_ml\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Gauss-Newton yÃ¶ntemi ile model oluÅŸturma\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_minimize.py:708\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    706\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 708\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_optimize.py:1477\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, c1, c2, hess_inv0, **unknown_options)\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1475\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m-> 1477\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[1;32m   1481\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_optimize.py:402\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    398\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n","Cell \u001b[0;32mIn[1], line 20\u001b[0m, in \u001b[0;36mlog_likelihood\u001b[0;34m(params, X, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_likelihood\u001b[39m(params, X, y):\n\u001b[0;32m---> 20\u001b[0m     a, b \u001b[38;5;241m=\u001b[39m params\n\u001b[1;32m     21\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X, a) \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     22\u001b[0m     residuals \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m-\u001b[39m y_pred\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"],"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error"}]},{"cell_type":"markdown","source":"# Support Vector Machine\n\n> Bir destek vektÃ¶r makinesi (SVM), hem sÄ±nÄ±flandÄ±rma hem de regresyon gÃ¶revleri iÃ§in kullanÄ±labilen denetimli bir makine Ã¶ÄŸrenimi algoritmasÄ±dÄ±r. Ä°ki sÄ±nÄ±fÄ±n Ã¶rnekleri arasÄ±ndaki en bÃ¼yÃ¼k farkla farklÄ± sÄ±nÄ±flarÄ± ayÄ±rmak iÃ§in bir hiper dÃ¼zlem kullanÄ±r. Bu, modelin genelleme yeteneÄŸini geliÅŸtirmeye yardÄ±mcÄ± olur.\n\n> SVM'nin Ã§alÄ±ÅŸma prensibi, veri noktalarÄ±nÄ± sÄ±nÄ±flar arasÄ±nda bir hiperdÃ¼zlemle (veya dÃ¼zlemle) en iyi ÅŸekilde ayÄ±rmaya Ã§alÄ±ÅŸÄ±rken, bu hiperdÃ¼zlemi belirlerken maksimum marjinal ayÄ±rma ilkesini kullanÄ±r. Marjin, sÄ±nÄ±flar arasÄ±ndaki en yakÄ±n veri noktalarÄ± ile hiperdÃ¼zlem arasÄ±ndaki mesafedir. SVM, bu marjini maksimize eden hiperdÃ¼zlemi seÃ§er, bÃ¶ylece yeni veri noktalarÄ± sÄ±nÄ±flandÄ±rÄ±lÄ±rken daha iyi genelleme yapabilir.\n> \n> SVM'nin avantajlarÄ± arasÄ±nda yÃ¼ksek boyutlu veri setleri Ã¼zerinde etkili performans, Ã§eÅŸitli kernel fonksiyonlarÄ±nÄ± kullanabilme esnekliÄŸi ve overfitting'e karÅŸÄ± direnÃ§ gibi Ã¶zellikler bulunur. SVM ayrÄ±ca doÄŸrusal olmayan veri setleri Ã¼zerinde de iyi performans gÃ¶sterebilir, bunun iÃ§in Ã§eÅŸitli kernel fonksiyonlarÄ± kullanÄ±lÄ±r.\n>\n> SVM'nin bir diÄŸer Ã¶nemli Ã¶zelliÄŸi, destek vektÃ¶rleri kullanarak modeli Ã¶zetlemesidir. Destek vektÃ¶rleri, sÄ±nÄ±flar arasÄ±ndaki marjinal ayÄ±rÄ±cÄ± hiperdÃ¼zleme en yakÄ±n olan veri noktalarÄ±dÄ±r. Bu destek vektÃ¶rleri, SVM modelinin optimum hiperdÃ¼zlemi belirlemesinde kilit rol oynar.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nveri = pd.read_csv(\"/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv\")\n\nprint(veri.head())\n\nfrom sklearn.model_selection import train_test_split\n\nX = veri.drop(\"default.payment.next.month\", axis=1)\ny = veri[\"default.payment.next.month\"]\n\nX_egitim, X_test, y_egitim, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_egitim = scaler.fit_transform(X_egitim)\nX_test = scaler.transform(X_test)\n\nfrom sklearn.svm import SVC\n\nsvm_modeli = SVC(kernel='linear')\nsvm_modeli.fit(X_egitim, y_egitim)\n\ndogruluk = svm_modeli.score(X_test, y_test)\nprint(\"Model DoÄŸruluÄŸu:\", dogruluk)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.147460Z","iopub.status.idle":"2024-02-04T13:44:52.147978Z","shell.execute_reply.started":"2024-02-04T13:44:52.147689Z","shell.execute_reply":"2024-02-04T13:44:52.147725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest\n\n> Rastgele orman, eÄŸitim sÄ±rasÄ±nda Ã§ok sayÄ±da karar aÄŸacÄ± oluÅŸturarak ve tek tek aÄŸaÃ§larÄ±n sÄ±nÄ±flarÄ±nÄ±n modu olan sÄ±nÄ±fÄ± Ã§Ä±kararak Ã§alÄ±ÅŸan, topluluk denetimli bir makine Ã¶ÄŸrenimi algoritmasÄ±dÄ±r. Verilerin bÃ¼yÃ¼k bir kÄ±smÄ± eksik olduÄŸunda doÄŸruluÄŸu korur.\n\n> Random Forest algoritmasÄ±, birden fazla karar aÄŸacÄ±nÄ±n (decision tree) oluÅŸturulmasÄ±nÄ± iÃ§erir. Her karar aÄŸacÄ±, rastgele Ã¶rneklerle (bootstrap sampling) ve rastgele Ã¶zelliklerle (feature sampling) eÄŸitilir. Bu ÅŸekilde, her bir karar aÄŸacÄ± farklÄ± bir alt kÃ¼me veri ve Ã¶zellik kÃ¼mesi Ã¼zerinde eÄŸitilir, bu da modele Ã§eÅŸitlilik katar.\n> \n> SÄ±nÄ±flandÄ±rma problemleri iÃ§in, Random Forest karar aÄŸaÃ§larÄ± genellikle Ã§oÄŸunluk oyu (majority vote) prensibiyle birleÅŸtirilir. Yani, her karar aÄŸacÄ±nÄ±n tahmin ettiÄŸi sÄ±nÄ±f deÄŸerlerinin ortalamasÄ± alÄ±narak final tahmin yapÄ±lÄ±r. Regresyon problemleri iÃ§in ise karar aÄŸaÃ§larÄ±nÄ±n tahmin ettiÄŸi deÄŸerlerin ortalamasÄ± alÄ±nÄ±r.\n> \n> Random Forest'Ä±n avantajlarÄ± arasÄ±nda yÃ¼ksek doÄŸruluk, overfitting'e karÅŸÄ± direnÃ§, farklÄ± Ã¶zellik tÃ¼rlerini (kategorik, sayÄ±sal) bir arada kullanabilme yeteneÄŸi ve dengesiz veri setlerinde etkili performans gÃ¶sterme kabiliyeti bulunur.","metadata":{}},{"cell_type":"code","source":"# Import libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Load data\n\ndf = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\n\n# Exploratory data analysis \nprint(df.shape)\nprint(df.describe())\n\n# Preprocess data\nX = df.drop('Outcome', axis=1) \ny = df['Outcome']\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\n\n# Evaluate model \ny_pred = rf.predict(X_test)\nprint('Accuracy:', accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.149610Z","iopub.status.idle":"2024-02-04T13:44:52.149961Z","shell.execute_reply.started":"2024-02-04T13:44:52.149796Z","shell.execute_reply":"2024-02-04T13:44:52.149814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network\n\n> Yapay Sinir AÄŸlarÄ± (Neural Networks), beyin biyolojisinden ilham alarak tasarlanmÄ±ÅŸ bir makine Ã¶ÄŸrenimi ve yapay zeka yÃ¶ntemidir. Bir sinir aÄŸÄ±, nÃ¶ron adÄ± verilen basit birimlerin birbirine baÄŸlÄ± olduÄŸu bir yapÄ±dÄ±r. Her bir nÃ¶ron, girdi verilerini alÄ±r, bu verileri aÄŸÄ±rlÄ±klarla Ã§arparak toplar, bir aktivasyon fonksiyonuna sokar ve Ã§Ä±ktÄ±yÄ± Ã¼retir.\n\n> Yapay sinir aÄŸlarÄ±, genellikle bir girdi katmanÄ±, bir veya daha fazla gizli katman ve bir Ã§Ä±ktÄ± katmanÄ±ndan oluÅŸur. Katmanlar arasÄ±ndaki baÄŸlantÄ±lar, aÄŸÄ±rlÄ±klar olarak adlandÄ±rÄ±lan parametrelerle belirlenir. EÄŸitim sÃ¼recinde, sinir aÄŸÄ±, girdi verileriyle beslenir ve belirlenen hedef Ã§Ä±ktÄ±ya yaklaÅŸmak iÃ§in aÄŸÄ±rlÄ±klarÄ± gÃ¼ncellemek iÃ§in geriye doÄŸru yayÄ±lÄ±m algoritmasÄ± kullanÄ±r.\n> \n> Yapay Sinir AÄŸlarÄ±, geniÅŸ bir uygulama yelpazesine sahiptir ve sÄ±nÄ±flandÄ±rma, regresyon, kÃ¼melenme, desen tanÄ±ma, doÄŸal dil iÅŸleme ve oyun yapay zekasÄ± gibi birÃ§ok alanda kullanÄ±lÄ±rlar. Derin Ã–ÄŸrenme (Deep Learning) adÄ± verilen ve Ã§ok katmanlÄ± sinir aÄŸlarÄ±nÄ±n kullanÄ±ldÄ±ÄŸÄ± bir alt dalÄ±yla birlikte, Ã¶zellikle bÃ¼yÃ¼k ve karmaÅŸÄ±k veri setleri Ã¼zerinde baÅŸarÄ±lÄ± sonuÃ§lar elde edilmiÅŸtir.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Load data\ndf = pd.read_csv('/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv')\n\n# Exploratory data analysis\nprint(df.shape) \nprint(df.head())\n\n# Preprocess data\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Custom neural network implementation\nmodel = Sequential()\nmodel.add(Dense(units=16, activation='relu', input_dim=X_train.shape[1]))\nmodel.add(Dense(units=8, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\n    \n# Train model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train, y_train, batch_size=32, epochs=50)\n\n# Evaluate model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(\"Test Loss:\", loss)\nprint(\"Test Accuracy:\", accuracy)\n# Model evaluation\n#print(confusion_matrix(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.151149Z","iopub.status.idle":"2024-02-04T13:44:52.151450Z","shell.execute_reply.started":"2024-02-04T13:44:52.151301Z","shell.execute_reply":"2024-02-04T13:44:52.151313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multiple Linear Regression\n\n> Ã‡oklu DoÄŸrusal Regresyon (Multiple Linear Regression), bir baÄŸÄ±mlÄ± deÄŸiÅŸkenin birden fazla baÄŸÄ±msÄ±z deÄŸiÅŸken tarafÄ±ndan aÃ§Ä±klanmaya Ã§alÄ±ÅŸÄ±ldÄ±ÄŸÄ± bir regresyon analizi yÃ¶ntemidir. Ã‡oklu doÄŸrusal regresyon, baÄŸÄ±mlÄ± deÄŸiÅŸkenin tahmin edilmesinde kullanÄ±lan baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin doÄŸrusal bir kombinasyonunu ifade eden bir denklemi kullanÄ±r.\n\n> Bu regresyon denklemi, baÄŸÄ±mlÄ± deÄŸiÅŸkenin beklenen deÄŸerini, baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin katsayÄ±larÄ± ve bir sabitin bir kombinasyonu olarak ifade eder.Tahminlerin doÄŸruluÄŸu, modelin uyumunun derecesi ve baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin baÄŸÄ±mlÄ± deÄŸiÅŸken Ã¼zerindeki etkisi gibi faktÃ¶rlere baÄŸlÄ±dÄ±r.\n\n> MSE (Mean Squared Error), bir regresyon modelinin tahminlerinin gerÃ§ek deÄŸerlerden ne kadar uzak olduÄŸunu Ã¶lÃ§en bir performans metriÄŸidir.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Veri setini yÃ¼kleme\ndf = pd.read_csv(\"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\")\n\n# BaÄŸÄ±mlÄ± deÄŸiÅŸken (y) ve baÄŸÄ±msÄ±z deÄŸiÅŸkenler (X) olarak ayÄ±rma\n# Preprocess data  \nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\n# Veriyi eÄŸitim ve test setlerine ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Linear regresyon modelini oluÅŸturma ve eÄŸitme\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Modeli test seti Ã¼zerinde deÄŸerlendirme\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.152932Z","iopub.status.idle":"2024-02-04T13:44:52.153255Z","shell.execute_reply.started":"2024-02-04T13:44:52.153097Z","shell.execute_reply":"2024-02-04T13:44:52.153111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NonLinear Regression\n\n> Nonlinear Regression, baÄŸÄ±mlÄ± ve baÄŸÄ±msÄ±z deÄŸiÅŸkenler arasÄ±ndaki iliÅŸkinin doÄŸrusal olmadÄ±ÄŸÄ± durumlar iÃ§in kullanÄ±lan bir regresyon analiz yÃ¶ntemidir. Bu yÃ¶ntem, doÄŸrusal olmayan bir modelin veriye uyarlanmasÄ± iÃ§in kullanÄ±lÄ±r. Genellikle, bir polinom ya da baÅŸka bir matematiksel fonksiyon kullanÄ±larak bu iliÅŸki modellenir. Bu yÃ¶ntem, veri setindeki karmaÅŸÄ±k iliÅŸkileri modellemek iÃ§in kullanÄ±lÄ±r ve doÄŸrusal regresyonun yetersiz kaldÄ±ÄŸÄ± durumlarda tercih edilir.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\n# Veri setini yÃ¼kleme\ndata = pd.read_csv(\"/kaggle/input/wine-dataset-for-clustering/wine-clustering.csv\")\n\n# BaÄŸÄ±mlÄ± deÄŸiÅŸken (y) ve baÄŸÄ±msÄ±z deÄŸiÅŸkenler (X) olarak ayÄ±rma\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\n# Veriyi eÄŸitim ve test setlerine ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# DoÄŸrusal olmayan regresyon modelini oluÅŸturma ve eÄŸitme\nmodel = SVR()\nmodel.fit(X_train, y_train)\n\n# Modeli test seti Ã¼zerinde deÄŸerlendirme\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Ortalama Kareler HatasÄ± (MSE):\", mse)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.154716Z","iopub.status.idle":"2024-02-04T13:44:52.155096Z","shell.execute_reply.started":"2024-02-04T13:44:52.154912Z","shell.execute_reply":"2024-02-04T13:44:52.154927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision trees\n\n> Karar aÄŸaÃ§larÄ±, birÃ§ok makine Ã¶ÄŸrenimi algoritmasÄ±ndan biridir ve sÄ±nÄ±flandÄ±rma ve regresyon problemlerinde kullanÄ±lÄ±r. Bir karar aÄŸacÄ±, bir veri kÃ¼mesindeki Ã¶zelliklere dayalÄ± olarak bir dizi karar dÃ¼ÄŸÃ¼mÃ¼ ve sonuÃ§larÄ± olan yaprak dÃ¼ÄŸÃ¼mleri iÃ§eren bir aÄŸaÃ§ yapÄ±sÄ±dÄ±r. Karar aÄŸaÃ§larÄ±, veri kÃ¼mesini belirli Ã¶zelliklere gÃ¶re bÃ¶lerek ve her dÃ¼ÄŸÃ¼mdeki en uygun bÃ¶lÃ¼nmeyi seÃ§erek veri kÃ¼mesini sÄ±nÄ±flandÄ±rmak veya regresyon yapmak iÃ§in kullanÄ±lÄ±r.\n\n> Karar aÄŸaÃ§larÄ±, verileri anlamak ve yorumlamak kolay olduÄŸu iÃ§in popÃ¼lerdir. AyrÄ±ca, karmaÅŸÄ±k iliÅŸkileri modelleyebilirler ve genellikle diÄŸer makine Ã¶ÄŸrenimi algoritmalarÄ±ndan daha hÄ±zlÄ± eÄŸitim sÃ¼relerine sahiptirler. Karar aÄŸaÃ§larÄ±, sÄ±nÄ±flandÄ±rma ve regresyon problemleri iÃ§in geniÅŸ bir uygulama yelpazesine sahiptir.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\ndf = pd.read_csv('/kaggle/input/uci-ml-datasets/hou_all.csv')\n\nX = df.drop('0', axis=1)\ny = df['0']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Degerlendir ve Tahmin Et\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n#Dogruluk degeri dondur\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\nprint(confusion_matrix(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.156198Z","iopub.status.idle":"2024-02-04T13:44:52.156501Z","shell.execute_reply.started":"2024-02-04T13:44:52.156344Z","shell.execute_reply":"2024-02-04T13:44:52.156356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Naive Bayes / Bayes Teoremi\n\n> Bayes Teoremi, bir olayÄ±n gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ±, bu olayÄ±n meydana gelmesine iliÅŸkin kanÄ±tlara dayanarak gÃ¼ncellemeyi saÄŸlayan bir formÃ¼l olarak tanÄ±mlanÄ±r.Naive Bayes, Bayes Teoremi'nin bir uygulamasÄ±dÄ±r ve sÄ±nÄ±flandÄ±rma problemlerinde sÄ±klÄ±kla kullanÄ±lÄ±r. Bu algoritma, belirli bir Ã¶rneÄŸin bir sÄ±nÄ±fa ait olma olasÄ±lÄ±ÄŸÄ±nÄ±, Ã¶zniteliklerin deÄŸerleri verildiÄŸinde Bayes Teoremi'ni kullanarak hesaplar. Ã–znitelikler arasÄ±nda baÄŸÄ±msÄ±zlÄ±k varsayÄ±mÄ± yapar, yani Ã¶znitelikler arasÄ±ndaki iliÅŸkileri gÃ¶z ardÄ± eder\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nimport pandas as pd\n\ndf = pd.read_csv('/kaggle/input/malware-executable-detection/uci_malware_detection.csv')\n\nX = df.drop('Label', axis=1)\ny = df['Label']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n#Naive Bayes SÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± BaÅŸlat\ngnb = GaussianNB()\n\ngnb.fit(X_train, y_train)\n\ny_pred = gnb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.157378Z","iopub.status.idle":"2024-02-04T13:44:52.157672Z","shell.execute_reply.started":"2024-02-04T13:44:52.157524Z","shell.execute_reply":"2024-02-04T13:44:52.157536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Descent Algorithm\n\n> Gradient descent (gradyan iniÅŸi), bir fonksiyonun minimumunu bulmak iÃ§in kullanÄ±lan bir optimizasyon algoritmasÄ±dÄ±r. Algoritma, mevcut konumda fonksiyonun gradyanÄ± (tÃ¼revi) ile doÄŸrudan ters yÃ¶nde hareket ederek minimuma doÄŸru ilerler. Bu iÅŸlem, gradyan vektÃ¶rÃ¼nÃ¼n negatifine doÄŸru kÃ¼Ã§Ã¼k adÄ±mlarla yapÄ±lÄ±r, bÃ¶ylece fonksiyon minimuma yaklaÅŸÄ±r. Gradient descent, makine Ã¶ÄŸrenmesinde model parametrelerini ayarlamak iÃ§in sÄ±kÃ§a kullanÄ±lan bir optimizasyon tekniÄŸidir.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import classification_report\n\ndf = pd.read_csv('/kaggle/input/bank-note-authentication-uci-data/BankNote_Authentication.csv')\n\nX = df.drop('class', axis=1)\ny = df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Gradient Descent SÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± BaÅŸlat\nclf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.158999Z","iopub.status.idle":"2024-02-04T13:44:52.159302Z","shell.execute_reply.started":"2024-02-04T13:44:52.159154Z","shell.execute_reply":"2024-02-04T13:44:52.159166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Boosting Machines (GBM)\n\n> Gradient Boosting Machines (GBM), genellikle karar aÄŸaÃ§larÄ±nÄ± bir araya getirerek gÃ¼Ã§lÃ¼ bir tahmin modeli oluÅŸturan bir makine Ã¶ÄŸrenmesi tekniÄŸidir. GBM, gradyan descent algoritmasÄ±nÄ± kullanarak aÄŸaÃ§larÄ± bir araya getirir. Her aÄŸaÃ§, Ã¶nceki aÄŸaÃ§larÄ±n hatalarÄ±nÄ± dÃ¼zeltmek iÃ§in eÄŸitilir.Bu sÃ¼reÃ§, hata azaldÄ±kÃ§a yeni aÄŸaÃ§lar ekleyerek devam eder.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\ndf = pd.read_csv('/kaggle/input/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')\n\nX = df.drop('class', axis=1)\ny = df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#Gradient Boosting Classifier BaÅŸlat\ngbm = GradientBoostingClassifier()\n\ngbm.fit(X_train, y_train)\n\ny_pred = gbm.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Model accuracy:', accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.160657Z","iopub.status.idle":"2024-02-04T13:44:52.160999Z","shell.execute_reply.started":"2024-02-04T13:44:52.160838Z","shell.execute_reply":"2024-02-04T13:44:52.160852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KÃ¼meleme (K-Means, HiyerarÅŸik KÃ¼meleme, Gausian KarÄ±ÅŸÄ±m Modeli)\n\n> KÃ¼meleme, bir veri kÃ¼mesindeki benzer Ã¶rnekleri gruplara ayÄ±rma iÅŸlemidir. KÃ¼meleme algoritmalarÄ±, veri noktalarÄ±nÄ± benzerliklerine gÃ¶re gruplandÄ±rmak iÃ§in kullanÄ±lÄ±r ve veri setindeki doÄŸal yapÄ±yÄ± tanÄ±mlamak ve keÅŸfetmek iÃ§in kullanÄ±lÄ±r.\n\n**1. K-Means KÃ¼meleme:**\n\n> K-Means kÃ¼meleme, veri noktalarÄ±nÄ± belirli sayÄ±da kÃ¼me veya k-merkezine gÃ¶re gruplandÄ±rÄ±r.\n> Algoritma, Ã¶nceden belirlenmiÅŸ k sayÄ±sÄ±nda kÃ¼me merkezlerini rastgele yerleÅŸtirir ve veri noktalarÄ±nÄ± bu merkezlere gÃ¶re gruplandÄ±rÄ±r.\n> ArdÄ±ndan, kÃ¼me merkezleri tekrar hesaplanÄ±r ve veri noktalarÄ± bu yeni merkezlere gÃ¶re yeniden gruplandÄ±rÄ±lÄ±r.\n> Bu iÅŸlem, kÃ¼me merkezlerinin hareket etmeyene kadar veya belirli bir iterasyon sayÄ±sÄ±na ulaÅŸÄ±lana kadar tekrarlanÄ±r.\n\n**2. HiyerarÅŸik KÃ¼meleme:**\n\n> HiyerarÅŸik kÃ¼meleme, veri noktalarÄ±nÄ± bir aÄŸaÃ§ yapÄ±sÄ± iÃ§inde hiyerarÅŸik olarak gruplandÄ±rÄ±r.\n> Bu algoritma, agglomeratif (birleÅŸtirici) ve bÃ¶lÃ¼cÃ¼ (ayÄ±rÄ±cÄ±) olmak Ã¼zere iki temel yaklaÅŸÄ±ma sahiptir.\n> Agglomeratif yÃ¶ntemde, her veri noktasÄ± bir kÃ¼me olarak baÅŸlar ve en yakÄ±n iki kÃ¼me birleÅŸtirilir.\n> BÃ¶lÃ¼cÃ¼ yÃ¶ntemde, tÃ¼m veri noktalarÄ± tek bir kÃ¼me olarak baÅŸlar ve her adÄ±mda kÃ¼me bÃ¶lÃ¼nÃ¼r.\n> Bu iÅŸlem, bir kÃ¼meleme aÄŸacÄ± oluÅŸturulana kadar veya belirli bir kÃ¼me sayÄ±sÄ±na ulaÅŸÄ±lana kadar devam eder.\n\n**3. Gauss KarÄ±ÅŸÄ±m Modeli (Gaussian Mixture Model - GMM):**\n\n> Gauss KarÄ±ÅŸÄ±m Modeli, veri noktalarÄ±nÄ± bir veya daha fazla Gauss daÄŸÄ±lÄ±mÄ±nÄ±n bir kombinasyonu olarak modellemek iÃ§in kullanÄ±lÄ±r.\n> GMM, her biri bir merkez ve bir kovaryans matrisine sahip olan bir dizi Gauss daÄŸÄ±lÄ±mÄ±ndan oluÅŸur.\n> GMM, veri noktalarÄ±nÄ± bu Gauss daÄŸÄ±lÄ±mlarÄ±na gÃ¶re en iyi uyacak ÅŸekilde gruplandÄ±rÄ±r.\n> Bu algoritma, verinin karmaÅŸÄ±k yapÄ±sÄ±nÄ± modellemek ve belirli bir veri setindeki farklÄ± bileÅŸenlerin katkÄ±sÄ±nÄ± anlamak iÃ§in kullanÄ±lÄ±r.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import silhouette_score\n\n# Veri setini yÃ¼kleme ve Ã¶n iÅŸleme\ndf = pd.read_csv('/kaggle/input/dry-bean-dataset/Dry_Bean.csv')\n\n# BaÄŸÄ±mlÄ± deÄŸiÅŸken (y) ve baÄŸÄ±msÄ±z deÄŸiÅŸkenler (X) olarak ayÄ±rma\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\n# Veri setini train ve test olarak ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# K-Means KÃ¼meleme\nkmeans = KMeans(n_clusters=5) #5 parÃ§aya ayrÄ±lÄ±r\nkmeans.fit(X_train) \nkmeans_labels = kmeans.predict(X_test)\nkmeans_silhouette_score = silhouette_score(X_test, kmeans_labels)\n\n# HiyerarÅŸik KÃ¼meleme\nhierarchical = AgglomerativeClustering(n_clusters=5)\nhierarchical_labels = hierarchical.fit_predict(X_test)\nhierarchical_silhouette_score = silhouette_score(X_test, hierarchical_labels)\n\n# Gausian KarÄ±ÅŸÄ±m Modeli\ngmm = GaussianMixture(n_components=5)\ngmm.fit(X_train)\ngmm_labels = gmm.predict(X_test)\ngmm_silhouette_score = silhouette_score(X_test, gmm_labels)\n\n# SonuÃ§larÄ± deÄŸerlendirme\nprint(\"K-Means Silhouette Score:\", kmeans_silhouette_score)\nprint(\"HiyerarÅŸik KÃ¼meleme Silhouette Score:\", hierarchical_silhouette_score)\nprint(\"Gausian KarÄ±ÅŸÄ±m Modeli Silhouette Score:\", gmm_silhouette_score)","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.161936Z","iopub.status.idle":"2024-02-04T13:44:52.162251Z","shell.execute_reply.started":"2024-02-04T13:44:52.162095Z","shell.execute_reply":"2024-02-04T13:44:52.162108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Expectation Maximization (EM)\n\n> Expectation Maximization (EM), denetimsiz Ã¶ÄŸrenme algoritmalarÄ±ndan biridir ve genellikle gizli deÄŸiÅŸkenlere sahip modellerin tahmininde kullanÄ±lÄ±r.\n> EM algoritmasÄ±, veri setindeki gÃ¶zlemlerin bir karÄ±ÅŸÄ±mÄ±nÄ± oluÅŸturan gizli bileÅŸenlerin (Ã¶rneÄŸin Gauss karÄ±ÅŸÄ±m modeli) parametrelerini tahmin etmek iÃ§in kullanÄ±lÄ±r.\n> EM algoritmasÄ±, iki ana adÄ±mdan oluÅŸur: Beklenti (Expectation) adÄ±mÄ± ve Maksimizasyon (Maximization) adÄ±mÄ±.\n> Beklenti adÄ±mÄ±nda, veri noktalarÄ± Ã¼zerindeki gizli deÄŸiÅŸkenlerin olasÄ± deÄŸerleri hesaplanÄ±r.\n> Maksimizasyon adÄ±mÄ±nda, veri seti Ã¼zerindeki gÃ¶zlemlerin parametreleri (Ã¶rneÄŸin, ortalama ve kovaryans matrisleri) maksimize edilir.\n> Bu iki adÄ±m iteratif olarak tekrarlanÄ±r ve EM algoritmasÄ±, veri setindeki gizli yapÄ±nÄ±n ve bileÅŸenlerin tahmin edilmesini saÄŸlar.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import accuracy_score\n\n# Veri setini yÃ¼kleme ve Ã¶n iÅŸleme\ndf = pd.read_csv('/kaggle/input/chronic-kidney-disease/new_model.csv')\n# BaÄŸÄ±mlÄ± deÄŸiÅŸken (y) ve baÄŸÄ±msÄ±z deÄŸiÅŸkenler (X) olarak ayÄ±rma\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\n# Veri setini train ve test olarak ayÄ±rma\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Expectation Maximization (EM)\nem = GaussianMixture(n_components=2) # Ä°ki sÄ±nÄ±f olduÄŸunu varsayalÄ±m\nem.fit(X_train)\nem_predictions = em.predict(X_test)\nem_accuracy = accuracy_score(y_test, em_predictions)\n\n# SonuÃ§larÄ± deÄŸerlendirme\nprint(\"EM Accuracy:\", em_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T13:44:52.163336Z","iopub.status.idle":"2024-02-04T13:44:52.163638Z","shell.execute_reply.started":"2024-02-04T13:44:52.163488Z","shell.execute_reply":"2024-02-04T13:44:52.163501Z"},"trusted":true},"execution_count":null,"outputs":[]}]}